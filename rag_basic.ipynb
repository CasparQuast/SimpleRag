{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ecee2d",
   "metadata": {},
   "source": [
    "# Einfaches RAG-System Tutorial\n",
    "\n",
    "Um das Setup möglichst einfach zu halten, werden im folgenden populäre Cloud Modelle genutzt, um den Rag-Workflow aufzubauen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997fdb6",
   "metadata": {},
   "source": [
    "## Download der Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53169336",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --quiet numpy docling ollama matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d68a2",
   "metadata": {},
   "source": [
    "# Dokument Laden und Parsen\n",
    "\n",
    "Als Beispiel für ein Dokument, aus welchem Informationen extrahiert werden sollen, die nicht im statischen Wissen eines LLM enthalten sind, wird der Halbjährige Financial Report von Thales genutzt.\n",
    "https://www.thalesgroup.com/sites/default/files/2025-09/Thales%20-%202025%20Half-Yearly%20Financial%20report_0_0.pdf (Zugriff 17.11.2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a448dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "def load_pdf_with_docling(path: str) -> str:\n",
    "    converter = DocumentConverter()\n",
    "    result = converter.convert(Path(path))\n",
    "    doc = result.document\n",
    "    markdown = doc.export_to_markdown()\n",
    "    return markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe77d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O thales_halfyear_2025.pdf \"https://www.thalesgroup.com/sites/default/files/2025-09/Thales%20-%202025%20Half-Yearly%20Financial%20report_0_0.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ababa84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"./thales_halfyear_2025.pdf\"\n",
    "raw_text = load_pdf_with_docling(PDF_PATH)\n",
    "\n",
    "len(raw_text), raw_text[:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2fd7f0",
   "metadata": {},
   "source": [
    "## Chunking \n",
    "In diesem Schritt wird der vom Parser gelieferte Rohtext in kleinere Textsegmente („Chunks“) zerlegt.  \n",
    "Die Chunks dienen später als Grundeinheiten für das Retrieval: Das Embedding-Modell berechnet für jeden Chunk einen Vektor, und bei einer Anfrage werden diejenigen Chunks gesucht, die im Vektorraum am ähnlichsten sind.\n",
    "\n",
    "Die Funktion `chunk_text` implementiert eine einfache, absatzbasierte Strategie:\n",
    "\n",
    "- Zunächst wird der Text grob in Absätze aufgeteilt (Trennung über Leerzeilen).\n",
    "- Absätze, deren Länge zwischen `min_chars` und `max_chars` liegt, werden direkt als eigene Chunks übernommen.\n",
    "- Sehr lange Absätze werden zusätzlich in kleinere Fenster zerlegt, indem Wort für Wort ein Chunk aufgebaut wird, bis die maximale Zeichenzahl (`max_chars`) erreicht ist.\n",
    "- Sehr kurze Textstücke werden verworfen, damit keine „Rauschen“-Chunks entstehen, die kaum Inhalt tragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb365c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, min_chars: int = 200, max_chars: int = 800):\n",
    "    \"\"\"\n",
    "    Einfache, absatzbasierte Chunking-Funktion für Markdown-Text.\n",
    "\n",
    "    - Der Eingabetext wird zunächst grob in Absätze zerlegt (Trennung über Leerzeilen).\n",
    "    - Absätze, deren Länge zwischen min_chars und max_chars liegt, werden direkt als Chunks übernommen.\n",
    "    - Absätze, die länger als max_chars sind, werden in kleinere Fenster (Chunks) zerlegt.\n",
    "    \"\"\"\n",
    "    raw_paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    for para in raw_paragraphs:\n",
    "        if len(para) <= max_chars:\n",
    "            if len(para) >= min_chars:\n",
    "                chunks.append(para)\n",
    "        else:\n",
    "            words = para.split()\n",
    "            current_words = []\n",
    "\n",
    "            for w in words:\n",
    "                current_words.append(w)\n",
    "                # Sobald der aktuelle Fenster-Text max_chars erreicht, wird ein Chunk abgeschlossen\n",
    "                if len(\" \".join(current_words)) >= max_chars:\n",
    "                    chunk = \" \".join(current_words)\n",
    "                    chunks.append(chunk)\n",
    "                    current_words = []\n",
    "\n",
    "            # Rest am Ende des Absatzes, der keinen eigenen vollen Chunk mehr bildet\n",
    "            if current_words:\n",
    "                tail = \" \".join(current_words)\n",
    "                # Nur übernehmen, wenn er nicht zu kurz ist oder bisher noch gar kein Chunk existiert\n",
    "                # (damit zumindest ein Chunk erzeugt wird)\n",
    "                if len(tail) >= min_chars or not chunks:\n",
    "                    chunks.append(tail)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "chunk_texts = chunk_text(raw_text)\n",
    "chunks = [{\"id\": i, \"text\": t} for i, t in enumerate(chunk_texts)]\n",
    "print(f\"Anzahl Chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c3a4b9",
   "metadata": {},
   "source": [
    "Für das Tutorial werden lokal betriebene Modelle verwendet: embeddinggemma:300m für Embeddings und gemma3:27b als LLM. Beide Modelle werden über Ollama geladen und ausgeführt, sodass keine externen Cloud-Dienste benötigt werden\n",
    "\n",
    "Alternativ zu gemma3 kann jedes andere Ollama unterstütze Model geladen werden. Siehe: https://ollama.com/search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ad111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "EMBEDDING_MODEL = \"embeddinggemma:300m\"\n",
    "CHAT_MODEL = \"gemma3:27b\"\n",
    "\n",
    "try:\n",
    "    _ = ollama.list()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Ollama scheint nicht zu laufen. Bitte Ollama-App oder 'ollama serve' starten.\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c092d",
   "metadata": {},
   "source": [
    "## Embeddings der Chunks mit `embeddinggemma:300m`\n",
    "\n",
    "- In diesem Schritt werden alle zuvor erzeugten Text-Chunks in Vektoren (Embeddings) überführt.\n",
    "- Dieser Index bildet die Grundlage für das spätere Retrieval (Top-k-Suche nach relevanten Chunks zur Nutzeranfrage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embed_texts(texts: list[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Erzeugt Embeddings lokal über Ollama mit embeddinggemma:300m.\n",
    "    Erwartet eine Liste von Texten und liefert ein numpy-Array (N x d) zurück.\n",
    "    \"\"\"\n",
    "    response = ollama.embed(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=texts,\n",
    "    )\n",
    "    vectors = response[\"embeddings\"]\n",
    "    return np.array(vectors, dtype=\"float32\")\n",
    "\n",
    "chunk_texts = [c[\"text\"] for c in chunks]\n",
    "chunk_embeddings = embed_texts(chunk_texts)\n",
    "\n",
    "print(\"Anzahl Chunks:\", len(chunks))\n",
    "print(\"Shape der Embeddings:\", chunk_embeddings.shape)\n",
    "\n",
    "index = {\n",
    "    \"chunks\": chunks,\n",
    "    \"embeddings\": chunk_embeddings,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cbbd65",
   "metadata": {},
   "source": [
    "### Retrieval über Cosine Similarity\n",
    "\n",
    "Die Funktion `cosine_similarity` berechnet die Ähnlichkeit zwischen der eingebetteten Nutzeranfrage und allen Chunk-Embeddings.  \n",
    "`retrieve` nutzt diese Ähnlichkeiten, sortiert sie absteigend und gibt die Top-k Chunks mit höchstem Score als Retrieval-Ergebnis zurück.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d9d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cosine Similarity zwischen einem Vektor a (1 x d) und vielen Vektoren b (N x d).\n",
    "    \"\"\"\n",
    "    a_norm = a / np.linalg.norm(a, axis=-1, keepdims=True)\n",
    "    b_norm = b / np.linalg.norm(b, axis=-1, keepdims=True)\n",
    "    return np.dot(a_norm, b_norm.T)\n",
    "\n",
    "\n",
    "def retrieve(query: str, index: dict, k: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Embed der Query, Cosine Similarity, Top-k-Chunks zurückgeben.\n",
    "    \"\"\"\n",
    "    q_emb = embed_texts([query])\n",
    "    sims = cosine_similarity(q_emb, index[\"embeddings\"])[0]\n",
    "\n",
    "    top_k_idx = np.argsort(sims)[-k:][::-1]\n",
    "\n",
    "    results = []\n",
    "    for i in top_k_idx:\n",
    "        results.append({\n",
    "            \"score\": float(sims[i]),\n",
    "            \"id\": index[\"chunks\"][i][\"id\"],\n",
    "            \"text\": index[\"chunks\"][i][\"text\"],\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7909d3",
   "metadata": {},
   "source": [
    "### Prompt-Konstruktion für das LLM\n",
    "\n",
    "`build_rag_prompt` kombiniert die Nutzerfrage mit den Top-k-Retrieval-Chunks zu einem einzigen Prompt.  \n",
    "Der Kontext wird dabei explizit eingefügt, und das Modell wird angewiesen, seine Antwort ausschließlich auf diese Textpassagen zu stützen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(question: str, retrieved_chunks: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Baut einen RAG-Prompt aus Frage + Top-k-Chunks.\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    for r in retrieved_chunks:\n",
    "        context_parts.append(f\"[Chunk {r['id']}] {r['text']}\")\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Du beantwortest Fragen ausschließlich auf Basis des bereitgestellten Kontextes.\n",
    "Wenn eine Information im Kontext nicht enthalten ist, sage ausdrücklich, dass sie nicht vorliegt.\n",
    "Erfinde keine Fakten.\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\n",
    "Frage:\n",
    "{question}\n",
    "\n",
    "Antwort (knapp, sachlich, auf Deutsch):\n",
    "\"\"\".strip()\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe2e91",
   "metadata": {},
   "source": [
    "### End-to-End-RAG-Aufruf\n",
    "\n",
    "`answer_question` führt den gesamten RAG-Durchlauf aus:  \n",
    "Zuerst werden per `retrieve` die Top-k-Chunks gefunden, daraus mit `build_rag_prompt` ein Prompt gebaut und dieser anschließend an `gemma3:27b` über Ollama geschickt. Die Funktion gibt die Modellantwort sowie die verwendeten Chunks zurück.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e42e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, index: dict, k: int = 5) -> tuple[str, list[dict]]:\n",
    "    \"\"\"\n",
    "    Vollständiger RAG-Durchlauf:\n",
    "    - Retrieval der Top-k-Chunks\n",
    "    - Prompt-Konstruktion\n",
    "    - Antwort von gemma3:27b über Ollama\n",
    "\n",
    "    Rückgabe: (Antworttext, verwendete Chunks)\n",
    "    \"\"\"\n",
    "    retrieved = retrieve(question, index, k=k)\n",
    "    prompt = build_rag_prompt(question, retrieved)\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Du bist ein hilfreicher Assistent für Fragen basierend auf den dir vorliegenden Informationen.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    answer = response[\"message\"][\"content\"]\n",
    "    return answer, retrieved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d5a18",
   "metadata": {},
   "source": [
    "### Beispielanfrage an das RAG-System\n",
    "\n",
    "Zum Abschluss wird eine Beispielfrage an das RAG-System gestellt.  \n",
    "Die Funktion `answer_question` liefert dabei sowohl die Antwort von `gemma3:27b` als auch die dafür herangezogenen Chunks, die zur Kontrolle mit Score und Chunk-ID ausgegeben werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae509f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Welche Veränderungen gab es in den Vorstandsgehältern oder der Vergütungsstruktur zwischen H1 2024 und H1 2025?\"\n",
    "answer, retrieved = answer_question(question, index, k=4)\n",
    "\n",
    "print(\"Frage:\\n\", question)\n",
    "print(\"\\nVerwendete Chunks:\\n\")\n",
    "for r in retrieved:\n",
    "    print(f\"- (Score {r['score']:.3f}, Chunk-ID {r['id']}):\\n{r['text'][:300]}...\\n\")\n",
    "\n",
    "print(\"Antwort von gemma3:27b:\\n\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559950cd",
   "metadata": {},
   "source": [
    "## QA-Benchmark\n",
    "\n",
    "Für den Test wird eine KI als Judge verwendet, um zu bewerten, ob die Modell antworten inhaltlich denen im Dataset entspricht.\n",
    "Dafür muss im folgenden ein API Key für OpenAI angegeben werden, wofür ein Account benötigt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d5ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"key\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba3876d",
   "metadata": {},
   "source": [
    "## Ausführen des Benchmarks\n",
    "\n",
    "Im Anschluss wird eine Grafik erzeugt, welche die korrekten Antwort (in %) pro Kategorie visualisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e666604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "with open(\"qa_benchmark.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_dataset = json.load(f)\n",
    "\n",
    "print(\"Loaded\", len(qa_dataset), \"Benchmark Items\")\n",
    "\n",
    "class JudgeScore(BaseModel):\n",
    "    score: float  # 0 bis 1\n",
    "\n",
    "\n",
    "def judge_answer(question: str, model_answer: str, gold_answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Bewertet die Modellantwort über GPT mithilfe eines\n",
    "    strikt erzwungenen JSON-Ausgabeformats (Pydantic).\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Bewerte die Modellantwort im Vergleich zur Goldstandard-Antwort inhaltlich.\n",
    "\n",
    "Kriterien:\n",
    "- Korrektheit\n",
    "- Vollständigkeit\n",
    "- Faktentreue\n",
    "\n",
    "Gib eine einzige Zahl zwischen 0 und 1 zurück:\n",
    "1.0 = vollständig korrekt\n",
    "0.5 = teilweise korrekt\n",
    "0.0 = falsch oder halluziniert\n",
    "\n",
    "Frage:\n",
    "{question}\n",
    "\n",
    "Goldstandard:\n",
    "{gold_answer}\n",
    "\n",
    "Modellantwort:\n",
    "{model_answer}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-5.1\",\n",
    "        input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        text_format=JudgeScore,\n",
    "        max_output_tokens=20\n",
    "    )\n",
    "\n",
    "    return float(response.output_parsed.score)\n",
    "\n",
    "results = []\n",
    "\n",
    "for item in qa_dataset:\n",
    "    q = item[\"question\"]\n",
    "    gold = item[\"answer\"]\n",
    "\n",
    "    model_answer, used_chunks = answer_question(q, index, k=4)\n",
    "    score = judge_answer(q, model_answer, gold)\n",
    "\n",
    "    results.append({\n",
    "        \"id\": item[\"id\"],\n",
    "        \"section\": item[\"section\"],\n",
    "        \"type\": item[\"type\"],\n",
    "        \"question\": q,\n",
    "        \"gold_answer\": gold,\n",
    "        \"model_answer\": model_answer,\n",
    "        \"score\": score,\n",
    "        \"used_chunks\": [c[\"id\"] for c in used_chunks],\n",
    "    })\n",
    "\n",
    "    print(f\"{item['id']} — Score: {score:.3f}\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"rag_eval_results.csv\", index=False)\n",
    "\n",
    "with open(\"rag_eval_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nEvaluation finished.\")\n",
    "print(\"Saved rag_eval_results.csv + rag_eval_results.json\")\n",
    "\n",
    "\n",
    "threshold = 0.85\n",
    "df[\"correct\"] = df[\"score\"] >= threshold\n",
    "\n",
    "type_stats = df.groupby(\"type\")[\"correct\"].mean() * 100\n",
    "type_stats = type_stats.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(type_stats.index, type_stats.values, color=\"#4a90e2\")\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        height + 1,\n",
    "        f\"{height:.1f}%\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.title(\"RAG-Answer-Quality pro Typ (in % korrekt beantwortet)\", fontsize=14)\n",
    "plt.ylabel(\"Korrekt beantwortet (%)\", fontsize=12)\n",
    "plt.ylim(0, 105)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
